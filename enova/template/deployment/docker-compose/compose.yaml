version: "3.8"
name: enova-mon

services:
  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.0-ubuntu22.04
    container_name: enova-dcgm-exporter
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    restart: always
    environment:
      - DCGM_EXPORTER_NO_HOSTNAME=1
    cap_add:
      - SYS_ADMIN
    ports:
      - "9400"
    volumes:
      - ./dcgm-exporter/default-counters.csv:/etc/dcgm-exporter/default-counters.csv:ro
    networks:
      - enova-net
      
  prometheus:
    image: prom/prometheus
    container_name: enova-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-remote-write-receiver'
    ports:
      - "32826:9090"
    restart: unless-stopped
    volumes:
      - ./prometheus:/etc/prometheus
      - prom_data:/prometheus
    networks:
      enova-net:
        aliases:
          - enova-prometheus

  grafana:
    image: grafana/grafana
    container_name: enova-grafana
    ports:
      - "32827:3000"
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=grafana
      - GF_FEATURE_TOGGLES_ENABLE=flameGraph traceqlSearch traceQLStreaming correlations metricsSummary traceqlEditor traceToMetrics traceToProfiles
    volumes:
      # - ./grafana:/etc/grafana/provisioning/datasources 
      # - grafana_data:/var/lib/grafana
      - ./grafana/grafana_provisioning:/etc/grafana/provisioning:ro
      - ./grafana/grafana_dashboards:/etc/dashboards:ro
    networks:
      - enova-net

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.93.0
    container_name: enova-otel-collector
    restart: always
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector/collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "8888"   # Prometheus metrics exposed by the collector
      - "8889"   # Prometheus exporter metrics
      - "32893:4317"   # OTLP gRPC receiver
      - "13133"  # health_check
      - "9090"
    networks:
      - enova-net

  tempo:
    image: grafana/tempo:latest
    container_name: enova-tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
    ports:
      - "3200"
      - "4317"  # otlp grpc
      - "4318"  # otlp http
    networks:
      - enova-net

  nginx:
    image: nginx:latest
    container_name: enova-nginx
    restart: always
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "9199:9199"

    networks:
      - enova-net

  webui-nginx:
    image: nginx:latest
    container_name: enova-webui-nginx
    restart: always
    volumes:
      - ./webui-nginx/nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "8501:8501"
    networks:
      - enova-net

  # haproxy:
  #   privileged: true
  #   image: haproxy:latest
  #   container_name: enova-haproxy
  #   volumes:
  #     - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
  #   ports:
  #     - "9199:9199"
  #   networks:
  #     - enova-net

  enova-algo:
    image: emergingai/enova:v0.0.2
    container_name: enova-algo
    command: "enova algo run"
    ports:
      - "8181:8181"
    networks:
      enova-net:
        aliases:
          - enova-algo

  enova-app:
    image: emergingai/enova:v0.0.2
    container_name: enova-app
    command: "enova app run"
    volumes:
      - /run/docker.sock:/run/docker.sock
      - /root/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    ports:
      - "8182:8182"
    depends_on:
      - enova-escaler
    networks:
      enova-net:
        aliases:
          - enova-app

  enova-escaler:
    image: emergingai/enova-escaler:v0.0.2  
    container_name: enova-escaler
    command: "bash /app/scripts/local_docker_run.sh --conf /etc/escaler/conf/settings.json"
    volumes:
      - ./escaler/conf/settings.json:/etc/escaler/conf/settings.json
      - /run/docker.sock:/run/docker.sock
    ports:
      - "8183:8183"
    networks:
      enova-net:
        aliases:
          - enova-escaler
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  traffic_injector:
    image: emergingai/enova-jmeter:v0.0.2
    command:
      - sh
      - -c
      - |
        rm -rf /data/report
        mkdir /data/report
        jmeter -n -t /data/jmeter-config.xml -l /data/report/report.log -e -o /data/report
    networks:
      - enova-net

volumes:
  prom_data:
  traffic_injector_dataset:
    driver_opts:
      type: none
      device: /home/traffic_injector_dataset
      o: bind

networks:
  enova-net:
    enable_ipv6: false

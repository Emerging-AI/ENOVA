from typing import Iterable

from opentelemetry import context as context_api
from opentelemetry import metrics
from opentelemetry.instrumentation.utils import _SUPPRESS_INSTRUMENTATION_KEY
from opentelemetry.metrics import CallbackOptions, Observation

meter = metrics.get_meter(__name__)

tokens_counter = meter.create_counter(
    name="generated_tokens",
    description="Counts the number of tokens generated by the forward method",
    unit="1"
)

engine_init_info = {
    "model": "",
    "tokenizer": "",
    "tokenizer_mode": "",
    "revision": "",
    "tokenizer_revision": "",
    "trust_remote_code": "",
    "dtype": "",
    "max_seq_len": "",
    "download_dir": "",
    "load_format": "",
    "tensor_parallel_size": "",
    "disable_custom_all_reduce": "",
    "quantization": "",
    "enforce_eager": "",
    "kv_cache_dtype": "",
    "seed": "",
    "max_num_batched_tokens": "",
    "max_num_seqs": "",
    "max_paddings": "",
    "pipeline_parallel_size": "",
    "worker_use_ray": "",
    "max_parallel_loading_workers": "",
}

engine_init_metric_name = "llm_engine_init_config"


def create_scrape_metric_callback(metric_name):
    def scrape_metric_callback(options: CallbackOptions) -> Iterable[Observation]:
        yield Observation(0, attributes=engine_init_info)

    return scrape_metric_callback


def _with_tracer_wrapper(func):
    """Helper for providing tracer for wrapper functions."""

    def _with_tracer(tracer, to_wrap):
        def wrapper(wrapped, instance, args, kwargs):
            return func(tracer, to_wrap, wrapped, instance, args, kwargs)

        return wrapper

    return _with_tracer

@_with_tracer_wrapper
def forward_wrapper(tracer, to_wrap, wrapped, instance, args, kwargs):
    if context_api.get_value(_SUPPRESS_INSTRUMENTATION_KEY):
        return wrapped(*args, **kwargs)

    span_name = to_wrap.get("span_name")

    with tracer.start_as_current_span(span_name) as span:

        input_ids = kwargs.get('input_ids', None)
        tokens_generated = 1
        if input_ids is not None:
            if input_ids.ndim > 1:
                batch_size = input_ids.shape[0]
                sequence_length = input_ids.shape[1]
                tokens_generated = batch_size
            else:
                batch_size = 1
                sequence_length = input_ids.shape[0]

            span.set_attribute("batch_size", batch_size)
            span.set_attribute("sequence_length", sequence_length)

        response = wrapped(*args, **kwargs)
        tokens_counter.add(tokens_generated)
        return response


def safe_getattr(obj, attr):
    value = getattr(obj, attr, None)
    if value is not None:
        return value
    return "None"


@_with_tracer_wrapper
def llmengine_init_wrapper(tracer, to_wrap, wrapped, instance, args, kwargs):
    if context_api.get_value(_SUPPRESS_INSTRUMENTATION_KEY):
        return wrapped(*args, **kwargs)

    response = wrapped(*args, **kwargs)

    model_config = getattr(instance, 'model_config', None)
    if model_config:
        engine_init_info['model'] = str(getattr(model_config, "model", ""))
        engine_init_info['tokenizer'] = str(getattr(model_config, "tokenizer", ""))
        engine_init_info['tokenizer_mode'] = str(getattr(model_config, "tokenizer_mode", ""))
        engine_init_info['revision'] = str(getattr(model_config, "revision", ""))
        engine_init_info['tokenizer_revision'] = str(getattr(model_config, "tokenizer_revision", ""))
        engine_init_info['trust_remote_code'] = safe_getattr(model_config, "trust_remote_code")
        engine_init_info['dtype'] = str(getattr(model_config, "dtype", ""))
        engine_init_info['max_seq_len'] = safe_getattr(model_config, "max_model_len")
        engine_init_info['download_dir'] = str(getattr(model_config, "download_dir", ""))
        engine_init_info['load_format'] = str(getattr(model_config, "load_format", ""))
        engine_init_info['quantization'] = str(getattr(model_config, "quantization", ""))
        engine_init_info['enforce_eager'] = safe_getattr(model_config, "enforce_eager")
        engine_init_info['seed'] = safe_getattr(model_config, "seed")

    schedule_config = getattr(instance, 'scheduler_config', None)
    if schedule_config:
        engine_init_info['max_num_batched_tokens'] = safe_getattr(schedule_config, "max_num_batched_tokens")
        engine_init_info['max_num_seqs'] = safe_getattr(schedule_config, "max_num_seqs")
        engine_init_info['max_paddings'] = safe_getattr(schedule_config, "max_paddings")

    cache_config = getattr(instance, 'cache_config', None)
    if cache_config:
        engine_init_info['kv_cache_dtype'] = str(getattr(cache_config, "kv_cache_dtype", ""))

    parallel_config = getattr(instance, 'parallel_config', None)
    if parallel_config:
        engine_init_info['tensor_parallel_size'] = safe_getattr(parallel_config, "tensor_parallel_size")
        engine_init_info['pipeline_parallel_size'] = safe_getattr(parallel_config, "pipeline_parallel_size")
        engine_init_info['disable_custom_all_reduce'] = safe_getattr(parallel_config, "disable_custom_all_reduce")
        engine_init_info['worker_use_ray'] = safe_getattr(parallel_config, "worker_use_ray")
        engine_init_info['max_parallel_loading_workers'] = safe_getattr(parallel_config, "max_parallel_loading_workers")

    callback = create_scrape_metric_callback(engine_init_metric_name)

    meter.create_observable_gauge(
        name=engine_init_metric_name,
        callbacks=[callback],
        description=f"The value of {engine_init_metric_name}",
    )

    return response

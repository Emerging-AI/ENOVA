#### [**English**](https://github.com/Emerging-AI/ENOVA) | [**ä¸­æ–‡**](https://github.com/Emerging-AI/ENOVA/blob/master/README_ZH.md)

# ENOVA 

<a href=''><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
[![Code License](https://img.shields.io/badge/Code%20License-Apache-green.svg)](https://github.com/Emerging-AI/ENOVA?tab=Apache-2.0-1-ov-file)


ENOVA æ˜¯ä¸€ç§ç”¨äº LLM éƒ¨ç½²ã€ç›‘æ§ã€æ³¨å…¥å’Œè‡ªåŠ¨æ‰©å±•çš„å¼€æºæœåŠ¡ã€‚
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åç«¯ç³»ç»Ÿçš„æ—¥ç›Šæ™®åŠï¼Œåœ¨ GPU é›†ç¾¤ä¸Šéƒ¨ç½²å…·æœ‰è‡ªåŠ¨æ‰©å±•åŠŸèƒ½çš„ç¨³å®šæ— æœåŠ¡å™¨ LLM æœåŠ¡å·²å˜å¾—è‡³å…³é‡è¦ã€‚
ç„¶è€Œï¼Œç”±äº GPU é›†ç¾¤ä¸­åº”ç”¨ç¨‹åºçš„å¤šæ ·æ€§å’Œå…±ç½®æ€§ï¼Œå‡ºç°äº†ä¸€äº›æŒ‘æˆ˜ï¼Œå¯¼è‡´æœåŠ¡è´¨é‡å’Œ GPU åˆ©ç”¨ç‡ä½ä¸‹ã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒENOVA è§£æ„äº† LLM æœåŠ¡æ‰§è¡Œè¿‡ç¨‹ï¼Œå¹¶ç»“åˆäº†ç”¨äºåœ¨ä»»ä½• GPU é›†ç¾¤ä¸Šè‡ªåŠ¨éƒ¨ç½²çš„é…ç½®æ¨èæ¨¡å—å’Œç”¨äºè‡ªåŠ¨æ‰©å±•çš„æ€§èƒ½æ£€æµ‹æ¨¡å—ã€‚
æ­¤å¤–ï¼ŒENOVA è¿˜å…·æœ‰éƒ¨ç½²æ‰§è¡Œå¼•æ“ï¼Œå¯å®ç°é«˜æ•ˆçš„ GPU é›†ç¾¤è°ƒåº¦ã€‚

é€šè¿‡ **ENOVA**, ç”¨æˆ·å¯ä»¥:
- ä»…éœ€å‡ è¡Œå‘½ä»¤å³å¯æ„å»ºå’Œéƒ¨ç½² LLM
- ä¸º LLM æ¨èæœ€ä½³è®¡ç®—èµ„æºå’Œè¿è¡Œå‚æ•°é…ç½®
- é€šè¿‡è¯·æ±‚æ³¨å…¥å¿«é€Ÿä½“éªŒ ENOVA çš„ LLM æ€§èƒ½
- æ·±å…¥è§‚å¯Ÿ LLM è¿è¡ŒçŠ¶æ€å’Œå¼‚å¸¸è‡ªæ„ˆ
- é€šè¿‡è‡ªåŠ¨æ‰©ç¼©å®ç°è´Ÿè½½å¹³è¡¡

ä»¥ä¸‹æ˜¯ ENOVA çš„æ ¸å¿ƒæŠ€æœ¯ç‚¹å’Œä»·å€¼ï¼š
- **é…ç½®æ¨è**ï¼šENOVA å¯ä»¥è‡ªåŠ¨è¯†åˆ«å„ç§ LLMï¼ˆå¼€æºæˆ–å¾®è°ƒï¼‰ï¼Œå¹¶æ¨èæœ€é€‚åˆéƒ¨ç½²æ¨¡å‹çš„å‚æ•°é…ç½®ï¼Œä¾‹å¦‚ GPU ç±»å‹ã€æœ€å¤§æ‰¹å¤„ç†å¤§å°ã€å‰¯æœ¬ã€æƒé‡ç­‰ã€‚
- **æ€§èƒ½æ£€æµ‹**ï¼šENOVA å¯ä»¥å®æ—¶ç›‘æ§æœåŠ¡è´¨é‡å’Œè®¡ç®—èµ„æºçš„å¼‚å¸¸ä½¿ç”¨æƒ…å†µã€‚
- **æ·±åº¦å¯è§‚æµ‹æ€§**ï¼šé€šè¿‡å¯¹å¤§å‹æ¨¡å‹çš„æ•´ä¸ªä»»åŠ¡æ‰§è¡Œé“¾è¿›è¡Œæ·±å…¥è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½å’Œä¼˜åŒ–è®¡ç®—èµ„æºçš„åˆ©ç”¨ç‡æä¾›æœ€ä½³æŒ‡å¯¼ã€‚
- **éƒ¨ç½²æ‰§è¡Œ**ï¼šå®ç°å¿«é€Ÿéƒ¨ç½²å’Œæ¨¡å‹æœåŠ¡ï¼Œä»¥è¾¾åˆ°è‡ªåŠ¨æ‰©å±•çš„ç›®æ ‡ã€‚


<p align="center">
<img src="./.github/assets/ENOVA.png">
</p>

åŸºäºä¸Šè¿°ENOVAçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿ä½¿ç”¨ENOVAçš„LLMæœåŠ¡ï¼š
- ç¨³å®šï¼šå®ç°99%ä»¥ä¸Šçš„é«˜å¯ç”¨ç‡ï¼Œç¡®ä¿ç¨³å®šè¿è¡Œè€Œä¸åœæœºã€‚
- æ€§ä»·æ¯”é«˜ï¼šèµ„æºåˆ©ç”¨ç‡æå‡50%ä»¥ä¸Šï¼Œç»¼åˆGPUå†…å­˜åˆ©ç”¨ç‡ä»40%æå‡åˆ°90%ã€‚
- é«˜æ•ˆï¼šéƒ¨ç½²æ•ˆç‡æå‡10å€ä»¥ä¸Šï¼Œä»¥æ›´ä½çš„å»¶è¿Ÿå’Œæ›´é«˜çš„ååé‡è¿è¡ŒLLM
- å¼ºå¤§çš„å¯æ‰©å±•æ€§ï¼šENOVAå¯ä»¥è‡ªåŠ¨å¯¹ä¸åŒç±»å‹çš„ä»»åŠ¡è¿›è¡Œèšç±»ï¼Œä»è€Œé€‚åº”è®¸å¤šé¢†åŸŸçš„åº”ç”¨ã€‚


## âœˆï¸ Getting Started

æˆ‘ä»¬å¯ä»¥åœ¨æ‚¨çš„GPUä¸Šå¿«é€Ÿè¿è¡Œå¼€æºAIæ¨¡å‹å¹¶è¿›è¡Œè¯·æ±‚æ³¨å…¥æµ‹è¯•ï¼Œä»¥å±•ç¤ºENOVAåœ¨æ¨¡å‹éƒ¨ç½²å’Œæ€§èƒ½ç›‘æ§æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚

### ç¯å¢ƒè¦æ±‚

1. æ“ä½œç³»ç»Ÿï¼šLinux
2. Docker
3. Pythonï¼š>=3.10
4. GPUï¼šè®¡ç®—èƒ½åŠ› 7.0 æˆ–æ›´é«˜çš„ Nvidia GPU

> [!NOTE]
> 
> å¦‚æœä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼ŒENOVA çš„å®‰è£…å’Œè¿è¡Œå¯èƒ½ä¼šå¤±è´¥ã€‚å¦‚æœæ‚¨æ²¡æœ‰å¯ç”¨çš„ GPU èµ„æºï¼Œ
> æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨ Google Colab ä¸Šçš„å…è´¹ GPU èµ„æºæ¥å®‰è£…å’Œä½“éªŒ ENOVAã€‚

### å®‰è£…

1. ç¡®ä¿æ‚¨æ‹¥æœ‰ [Docker](https://docs.docker.com/engine/install/) 
å’Œ [Python](https://docs.anaconda.com/free/anaconda/install/index.html) ç¯å¢ƒ.

2. ä½¿ç”¨pipå®‰è£…ENOVA:
```bash
# Create a new Python environment
conda create -n enova_env python=3.10
conda activate enova_env

# Install ENOVA
# Source: https://pypi.python.org/simple/
pip install enova_instrumentation_llmo
pip install enova
```

3. æ‚¨å¯ä»¥ä½¿ç”¨æ­¤å‘½ä»¤è¡Œæ£€æŸ¥å®‰è£…æ˜¯å¦æˆåŠŸ:
```bash
enova -h 
```

é¢„æœŸè¾“å‡ºä¸ºï¼š
```text
Usage: enova [OPTIONS] COMMAND [ARGS]...

  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
  â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•   â•šâ•â•â•â•  â•šâ•â•  â•šâ•â•

  ENOVA is an open-source llm deployment, monitoring, injection and auto-scaling service.
  It provides a set of commands to deploy stable serverless serving of LLM on GPU clusters with auto-scaling.

Options:
  -v, --version  Show the version and exit.
  -h, --help     Show this message and exit.

Commands:
  algo      Run the autoscaling service.
  app       Start ENOVA application server.
  enode     Deploy the target LLM and launch the LLM API service.
  injector  Run the autoscaling service.
  mon       Run the monitors of LLM server
  pilot     Start an all-in-one LLM server with deployment, monitoring,...
  webui     Build agent at this page based on the launched LLM API service.
```

### å¿«é€Ÿå¼€å§‹

1. å¯åŠ¨ä¸€ä¸ªé›†éƒ¨ç½²ã€ç›‘æ§ã€è¯·æ±‚æ³¨å…¥å’Œè‡ªåŠ¨æ‰©å±•æœåŠ¡äºä¸€ä½“çš„LLMæœåŠ¡å™¨ï¼š

```bash
enova pilot run --model mistralai/Mistral-7B-Instruct-v0.1

# openai
enova pilot run --model mistralai/Mistral-7B-Instruct-v0.1 --vllm_mode openai
```

å¦‚æœæ‚¨çš„GPUè®¾ç½®äº†ä»£ç†ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œï¼š

```bash
enova pilot run --model mistralai/Mistral-7B-Instruct-v0.1 --hf_proxy xxx
```

> [!TIP]
> 
> - LLM APIæœåŠ¡çš„é»˜è®¤ç«¯å£æ˜¯9199.
> - GrafanaæœåŠ¡çš„é»˜è®¤ç«¯å£æ˜¯32827.
> - LLM WebUIæœåŠ¡çš„é»˜è®¤ç«¯å£æ˜¯8501.
> - LLMéƒ¨ç½²å®Œæˆåçš„åº”ç”¨æœåŠ¡çš„é»˜è®¤ç«¯å£æ˜¯8182.


2. é€šè¿‡ENOVAåº”ç”¨æœåŠ¡å™¨æ£€æŸ¥å·²éƒ¨ç½²çš„LLMæœåŠ¡:

```text
http://localhost:8182/instance
```

<p align="center">
<img src="./.github/assets/llm_instance.png">
</p>


3. ä½¿ç”¨æç¤ºè¯æµ‹è¯•å·²éƒ¨ç½²çš„LLMæœåŠ¡:

ä½¿ç”¨WebUI:

```text
http://localhost:8501
```

ä½¿ç”¨Shell:

```bash
curl -X POST http://localhost:9199/generate \
-d '{
"prompt": "San Francisco is a",
"max_tokens": 1024,
"temperature": 0.9,
"top_p": 0.9
}'

# openai
curl http://localhost:9199/v1/completions \
-H "Content-Type: application/json" \
-d '{
"model": "mistralai/Mistral-7B-Instruct-v0.1",
"prompt": "San Francisco is a",
"max_tokens": 128,
"temperature": 0
}'
```

4. ä½¿ç”¨ ENOVA åº”ç”¨æœåŠ¡å™¨ç›‘æ§ LLM æœåŠ¡è´¨é‡ï¼Œæä¾›è·¨æœåŠ¡ã€æ¨¡å‹ã€èŠ‚ç‚¹å’Œç¡¬ä»¶å±‚çš„æ€§èƒ½å…¨é¢è§†å›¾ã€‚è‡ªåŠ¨æ‰©å±•å’Œé…ç½®æ›´æ–°å°†æ ¹æ®å®æ—¶è§‚å¯Ÿè‡ªåŠ¨æ‰§è¡Œã€‚

```text
http://localhost:8182/instance
```

<p align="center">
<img src="./.github/assets/monitoring_metrics.png">
</p>

5. å…³é—­æ‰€æœ‰æœåŠ¡
```
enova pilot stop --service all
```


## ğŸ  LLM éƒ¨ç½²

é™¤äº†æä¾›æœåŠ¡éƒ¨ç½²ã€ç›‘æ§å’Œè‡ªåŠ¨æ‰©å±•çš„ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆå¤–ï¼ŒENOVA è¿˜æ”¯æŒå•ç‹¬çš„æ¨¡å—ã€‚

LLM éƒ¨ç½²æœåŠ¡ç®€åŒ–äº† LLM çš„éƒ¨ç½²ï¼Œå¹¶æä¾›äº†ç¨³å®šçš„ API ä»¥å®ç°æ— ç¼è®¿é—®ã€‚

### è¿è¡Œ LLM

```bash
enova enode run --model mistralai/Mistral-7B-Instruct-v0.1
```

> [!NOTE]
> 
> LLM æœåŠ¡å™¨ä½¿ç”¨é»˜è®¤çš„ vllm åç«¯å¯åŠ¨ã€‚OpenAI API å’Œ Generate API å‡å—æ”¯æŒã€‚å¯ä»¥ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®š vllm é…ç½®ï¼Œä¾‹å¦‚:
```bash
enova enode run --model mistralai/Mistral-7B-Instruct-v0.1 --host 127.0.0.1 --port 9199
```


### LLM WebUI æœåŠ¡

è¯¥æœåŠ¡å…·æœ‰ç”¨äºå¯¹è¯äº¤äº’çš„WebUIé¡µé¢ï¼Œå…¶ä¸­LLMæœåŠ¡å™¨çš„æœåŠ¡ä¸»æœºå’Œç«¯å£ä»¥åŠWebUIæœåŠ¡çš„ä¸»æœºå’Œç«¯å£æ˜¯å¯é…ç½®å‚æ•°ã€‚

```bash
enova webui run --serving_host 127.0.0.1 --serving_port 9199 --host 127.0.0.1 --port 8501
```

<p align="center">
<img src="./.github/assets/webui.png">
</p>


### è‡ªåŠ¨æ‰©ç¼©å®¹

è‡ªåŠ¨ä¼¸ç¼©æœåŠ¡ç”± escaler æ¨¡å—è‡ªåŠ¨å¯åŠ¨å’Œç®¡ç†ï¼Œæˆ‘ä»¬çš„å¼€æºç‰ˆæœ¬æ”¯æŒå•èŠ‚ç‚¹è‡ªåŠ¨ä¼¸ç¼©ï¼Œå¦‚æœæ‚¨éœ€è¦æ›´å¤§é›†ç¾¤çš„è‡ªåŠ¨ä¼¸ç¼©ï¼Œè¯·é€šè¿‡å®˜ç½‘ã€GitHubã€Slack ä¸æˆ‘ä»¬è”ç³»ã€‚

### è¯·æ±‚æ³¨å…¥æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨ JMeter å®ç°äº†ä¸€ä¸ªè¯·æ±‚æ³¨å…¥æ¨¡å—ï¼Œä»¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¯·æ±‚æ¥è¯„ä¼° LLM æ€§èƒ½ã€‚è¯¥æ¨¡å—å…è®¸ä½¿ç”¨ä¸¤ç§æ¨¡å¼æ¨¡æ‹Ÿè¯·æ±‚åˆ°è¾¾æ¦‚ç‡ï¼šæ³Šæ¾åˆ†å¸ƒå’Œæ­£æ€åˆ†å¸ƒã€‚æœ‰å…³æ³¨å…¥æ“ä½œçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼š
```text
http://localhost:8182/instance
```

<p align="center">
<img src="./.github/assets/request_inject.png">
</p>

<p align="center">
<img src="./.github/assets/test_results.png">
</p>


## ğŸ  LLM ç›‘æ§æœåŠ¡ 

### ç®¡ç† LLM ç›‘æ§æœåŠ¡

ç›‘æ§ç³»ç»Ÿä¸“ä¸ºç›‘æ§å’Œè‡ªåŠ¨æ‰©å±•è€Œè®¾è®¡ï¼ŒåŒ…å«å®æ—¶æ•°æ®æ”¶é›†ã€å­˜å‚¨å’Œä½¿ç”¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç®¡ç† LLM ç›‘æ§æœåŠ¡ï¼š

1. å¯åŠ¨ LLM ç›‘æ§æœåŠ¡

```bash
enova mon start
```

2. æ£€æŸ¥æœåŠ¡çŠ¶æ€ 

```bash
enova mon status
```

3. åœæ­¢ LLM ç›‘æ§æœåŠ¡

```bash
enova mon stop
```


### ç›‘æ§æŒ‡æ ‡

ç›‘æ§æŒ‡æ ‡ä½¿ç”¨ DCGM å¯¼å‡ºå™¨ã€Prometheus å¯¼å‡ºå™¨å’Œ OpenTelemetry æ”¶é›†å™¨è¿›è¡Œæ”¶é›†ã€‚ä¸‹è¡¨æä¾›äº†ç®€è¦è¯´æ˜ã€‚
å¯ä»¥åœ¨æˆ‘ä»¬çš„åº”ç”¨æœåŠ¡å™¨ä¸ŠæŸ¥çœ‹æ›´è¯¦ç»†çš„æŒ‡æ ‡ï¼š


| Metric Type     | Metric Description                                              |
|-----------------|-----------------------------------------------------------------|
| API Service     | The number of requests sent to LLM services per second          |
| API Service     | The number of requests processed by LLM services per second     |
| API Service     | The number of requests successfully processed per second        |
| API Service     | The success rate of requests processed by LLM services per second |
| API Service     | The number of requests being processed by LLM services          |
| API Service     | The average execution time per request processed by LLM services |
| API Service     | The average request size of requests per second                 |
| API Service     | The average response size of requests per second                |
| LLM Performance | The average prompt throughput per second                        |
| LLM Performance | The average generation throughput per second                    |
| LLM Performance | The number of requests being processed by the deployed LLM      |
| LLM Performance | The number of requests being pended by the deployed LLM         |                           
| LLM Performance | The utilization ratio of memory allocated for KV cache          | 
| GPU Utilization | DCGM Metrics, like DCGM_FI_DEV_GPU_UTIL.                        |


åœ¨éƒ¨ç½²ENOVAä¸€ä½“åŒ–llmæœåŠ¡çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜åœ¨Grafanaä¸­åˆ›å»ºäº†ç›¸åº”çš„æŒ‡æ ‡ä»ªè¡¨ç›˜ï¼Œæ”¯æŒæŸ¥çœ‹æ›´è¯¦ç»†çš„traceæ•°æ®ã€‚

- URLï¼šhttp://localhost:32827/dashboards
- é»˜è®¤ç”¨æˆ·è´¦å·ï¼šadmin
- å¯†ç ï¼šgrafana


#### GPU æŒ‡æ ‡æ¡ˆä¾‹

<p align="center">
<img src="./.github/assets/gpu_metrics.png">
</p>

#### Traces æ¡ˆä¾‹

<p align="center">
<img src="./.github/assets/trace.png">
</p>


## ğŸ“š å‚è€ƒ

```text
@inproceedings{tao2024ENOVA,
  title={ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM Serving},
  author={Tao Huang and Pengfei Chen and Kyoka Gong and Jocky Hawk and Zachary Bright and Wenxin Xie and Kecheng Huang and Zhi Ji},
  booktitle={arXiv preprint arXiv:},
  year={2024}
}
```

## ğŸ¤ åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒº

æˆ‘ä»¬ä½¿ç”¨ Slack å·¥ä½œåŒºåä½œæ„å»º ENOVA

* [Slack workspace](https://join.slack.com/t/emergingai/shared_invite/zt-2i9ngqa10-OU8SsVJbV0mqTBrjjt5rmQ)
* é€šè¿‡æˆ‘ä»¬çš„å®˜ç½‘è·å–æ›´å¤šä¿¡æ¯ï¼šhttps://www.emergingai.pro
